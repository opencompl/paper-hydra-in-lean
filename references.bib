@article{grosser2015polyast,
 author = {Grosser, Tobias and Verdoolaege, Sven and Cohen, Albert},
 title = {Polyhedral {AST} Generation Is More Than Scanning Polyhedra},
 journal = {ACM Trans. Program. Lang. Syst.},
 issue_date = {July 2015},
 volume = {37},
 number = {4},
 month = jul,
 year = {2015},
 issn = {0164-0925},
 pages = {12:1--12:50},
 articleno = {12},
 numpages = {50},
 url = {http://doi.acm.org/10.1145/2743016},
 doi = {10.1145/2743016},
 acmid = {2743016},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Polyhedral compilation, Presburger relations, code generation, index set splitting, unrolling},
} 

@article{mckeeman_peephole_1965,
	title = {Peephole optimization},
	volume = {8},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/364995.365000},
	doi = {10.1145/364995.365000},
	abstract = {Redundant instructions may be discarded during the final stage of compilation by using a simple optimizing technique called peephole optimization. The method is described and examples are given.},
	language = {en},
	number = {7},
	urldate = {2024-11-11},
	journal = {Communications of the ACM},
	author = {McKeeman, W. M.},
	month = jul,
	year = {1965},
	pages = {443--444},
}

@inproceedings{lattner_llvm_2004,
	address = {San Jose, CA, USA},
	title = {{LLVM}: {A} compilation framework for lifelong program analysis \& transformation},
	isbn = {978-0-7695-2102-2},
	shorttitle = {{LLVM}},
	url = {http://ieeexplore.ieee.org/document/1281665/},
	doi = {10.1109/CGO.2004.1281665},
	abstract = {This paper describes LLVM (Low Level Virtual Machine), a compiler framework designed to support transparent, lifelong program analysis and transformation for arbitrary programs, by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. LLVM deﬁnes a common, low-level code representation in Static Single Assignment (SSA) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and eﬃciently. The LLVM compiler framework and code representation together provide a combination of key capabilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We describe the design of the LLVM representation and compiler framework, and evaluate the design in three ways: (a) the size and eﬀectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative examples of the beneﬁts LLVM provides for several challenging compiler problems.},
	language = {en},
	urldate = {2024-11-12},
	booktitle = {International {Symposium} on {Code} {Generation} and {Optimization}, 2004. {CGO} 2004.},
	publisher = {IEEE},
	author = {Lattner, C. and Adve, V.},
	year = {2004},
	pages = {75--86},
}

@inproceedings{menendez_termination-checking_2016,
	address = {New York, NY, USA},
	series = {{ICSE} '16},
	title = {Termination-checking for {LLVM} peephole optimizations},
	isbn = {978-1-4503-3900-1},
	url = {https://dl.acm.org/doi/10.1145/2884781.2884809},
	doi = {10.1145/2884781.2884809},
	abstract = {Mainstream compilers contain a large number of peephole optimizations, which perform algebraic simplification of the input program with local rewriting of the code. These optimizations are a persistent source of bugs. Our recent research on Alive, a domain-specific language for expressing peephole optimizations in LLVM, addresses a part of the problem by automatically verifying the correctness of these optimizations and generating C++ code for use with LLVM.This paper identifies a class of non-termination bugs that arise when a suite of peephole optimizations is executed until a fixed point. An optimization can undo the effect of another optimization in the suite, which results in non-terminating compilation. This paper (1) proposes a methodology to detect non-termination bugs with a suite of peephole optimizations, (2) identifies the necessary condition to ensure termination while composing peephole optimizations, and (3) provides debugging support by generating concrete input programs that cause non-terminating compilation. We have discovered 184 optimization sequences, involving 38 optimizations, that cause non-terminating compilation in LLVM with Alive-generated C++ code.},
	urldate = {2025-06-07},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Menendez, David and Nagarakatte, Santosh},
	month = may,
	year = {2016},
	pages = {191--202},
}

@article{yang_finding_2011,
	title = {Finding and understanding bugs in {C} compilers},
	volume = {46},
	issn = {0362-1340},
	url = {https://dl.acm.org/doi/10.1145/1993316.1993532},
	doi = {10.1145/1993316.1993532},
	abstract = {Compilers should be correct. To improve the quality of C compilers, we created Csmith, a randomized test-case generation tool, and spent three years using it to find compiler bugs. During this period we reported more than 325 previously unknown bugs to compiler developers. Every compiler we tested was found to crash and also to silently generate wrong code when presented with valid input. In this paper we present our compiler-testing tool and the results of our bug-hunting study. Our first contribution is to advance the state of the art in compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. Our second contribution is a collection of qualitative and quantitative results about the bugs we have found in open-source C compilers.},
	number = {6},
	urldate = {2025-05-24},
	journal = {SIGPLAN Not.},
	author = {Yang, Xuejun and Chen, Yang and Eide, Eric and Regehr, John},
	month = jun,
	year = {2011},
	pages = {283--294},
}

@misc{sasnauskas_souper_2018,
	title = {Souper: {A} {Synthesizing} {Superoptimizer}},
	shorttitle = {Souper},
	url = {http://arxiv.org/abs/1711.04422},
	doi = {10.48550/arXiv.1711.04422},
	abstract = {If we can automatically derive compiler optimizations, we might be able to sidestep some of the substantial engineering challenges involved in creating and maintaining a highquality compiler. We developed Souper, a synthesizing superoptimizer, to see how far these ideas might be pushed in the context of LLVM. Along the way, we discovered that Souper’s intermediate representation was sufﬁciently similar to the one in Microsoft Visual C++ that we applied Souper to that compiler as well. Shipping, or about-to-ship, versions of both compilers contain optimizations suggested by Souper but implemented by hand. Alternately, when Souper is used as a fully automated optimization pass it compiles a Clang compiler binary that is about 3 MB (4.4\%) smaller than the one compiled by LLVM.},
	language = {en},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Sasnauskas, Raimondas and Chen, Yang and Collingbourne, Peter and Ketema, Jeroen and Lup, Gratian and Taneja, Jubi and Regehr, John},
	month = apr,
	year = {2018},
	note = {arXiv:1711.04422 [cs]},
	keywords = {Computer Science - Programming Languages},
}

@article{lopes_provably_2015,
	title = {Provably correct peephole optimizations with alive},
	volume = {50},
	issn = {0362-1340},
	url = {https://dl.acm.org/doi/10.1145/2813885.2737965},
	doi = {10.1145/2813885.2737965},
	abstract = {Compilers should not miscompile. Our work addresses problems in developing peephole optimizations that perform local rewriting to improve the efficiency of LLVM code. These optimizations are individually difficult to get right, particularly in the presence of undefined behavior; taken together they represent a persistent source of bugs. This paper presents Alive, a domain-specific language for writing optimizations and for automatically either proving them correct or else generating counterexamples. Furthermore, Alive can be automatically translated into C++ code that is suitable for inclusion in an LLVM optimization pass. Alive is based on an attempt to balance usability and formal methods; for example, it captures---but largely hides---the detailed semantics of three different kinds of undefined behavior in LLVM. We have translated more than 300 LLVM optimizations into Alive and, in the process, found that eight of them were wrong.},
	number = {6},
	urldate = {2025-05-24},
	journal = {ACM SIGPLAN Notices},
	author = {Lopes, Nuno P. and Menendez, David and Nagarakatte, Santosh and Regehr, John},
	month = jun,
	year = {2015},
	pages = {22--32},
}

@inproceedings{lopes_alive2_2021,
	address = {New York, NY, USA},
	series = {{PLDI} 2021},
	title = {Alive2: bounded translation validation for {LLVM}},
	isbn = {978-1-4503-8391-2},
	shorttitle = {Alive2},
	url = {https://dl.acm.org/doi/10.1145/3453483.3454030},
	doi = {10.1145/3453483.3454030},
	abstract = {We designed, implemented, and deployed Alive2: a bounded translation validation tool for the LLVM compiler’s intermediate representation (IR). It limits resource consumption by, for example, unrolling loops up to some bound, which means there are circumstances in which it misses bugs. Alive2 is designed to avoid false alarms, is fully automatic through the use of an SMT solver, and requires no changes to LLVM. By running Alive2 over LLVM’s unit test suite, we discovered and reported 47 new bugs, 28 of which have been fixed already. Moreover, our work has led to eight patches to the LLVM Language Reference—the definitive description of the semantics of its IR—and we have participated in numerous discussions with the goal of clarifying ambiguities and fixing errors in these semantics. Alive2 is open source and we also made it available on the web, where it has active users from the LLVM community.},
	urldate = {2025-05-24},
	booktitle = {Proceedings of the 42nd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Lopes, Nuno P. and Lee, Juneyoung and Hur, Chung-Kil and Liu, Zhengyang and Regehr, John},
	month = jun,
	year = {2021},
	pages = {65--79},
}

@article{mukherjee_hydra_2024,
	title = {Hydra: {Generalizing} {Peephole} {Optimizations} with {Program} {Synthesis}},
	volume = {8},
	issn = {2475-1421},
	shorttitle = {Hydra},
	url = {https://dl.acm.org/doi/10.1145/3649837},
	doi = {10.1145/3649837},
	abstract = {Optimizing compilers rely on peephole optimizations to simplify  
combinations of instructions and remove redundant instructions.  
Typically, a new peephole optimization is added when a compiler  
developer notices an optimization opportunity---a collection of  
dependent instructions that can be improved---and manually derives a  
more general rewrite rule that optimizes not only the original code,  
but also other, similar collections of instructions.  
In this paper, we present Hydra, a tool that automates the process of  
generalizing peephole optimizations using a collection of techniques  
centered on program synthesis.  
One of the most important problems we have solved is finding a version  
of each optimization that is independent of the bitwidths of the  
optimization's inputs (when this version exists).  
We show that Hydra can generalize 75\% of the ungeneralized missed  
peephole optimizations that LLVM developers have posted to the LLVM  
project's issue tracker.  
All of Hydra's generalized peephole optimizations have been formally  
verified, and furthermore we can automatically turn them into C++ code  
that is suitable for inclusion in an LLVM pass.},
	language = {en},
	number = {OOPSLA1},
	urldate = {2024-11-03},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Mukherjee, Manasij and Regehr, John},
	month = apr,
	year = {2024},
	pages = {725--753},
}

@inproceedings{barbosa_challenges_2023,
	address = {Norway},
	title = {Challenges in {SMT} {Proof} {Production} and {Checking} for {Arithmetic} {Reasoning} ({Invited} {Paper})},
	url = {https://api.semanticscholar.org/CorpusID:260957118},
	booktitle = {8th {International} {Workshop} on {Satisfiability} {Checking} and {Symbolic} {Computation}},
	author = {Barbosa, Haniel},
	year = {2023},
}

@inproceedings{de_moura_z3_2008,
	address = {Berlin, Heidelberg},
	series = {{TACAS}'08/{ETAPS}'08},
	title = {Z3: an efficient {SMT} solver},
	isbn = {978-3-540-78799-0},
	shorttitle = {Z3},
	abstract = {Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.},
	urldate = {2024-11-13},
	booktitle = {Proceedings of the {Theory} and practice of software, 14th international conference on {Tools} and algorithms for the construction and analysis of systems},
	publisher = {Springer-Verlag},
	author = {De Moura, Leonardo and Bjørner, Nikolaj},
	month = mar,
	year = {2008},
	pages = {337--340},
}

@article{winterer_validating_2024,
	title = {Validating {SMT} {Solvers} for {Correctness} and {Performance} via {Grammar}-{Based} {Enumeration}},
	volume = {8},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3689795},
	doi = {10.1145/3689795},
	abstract = {DOMINIK WINTERER, ETH Zurich, Switzerland ZHENDONG SU, ETH Zurich, Switzerland We introduce ET, a grammar-based enumerator for validating SMT solver correctness and performance. By compiling grammars of the SMT theories to algebraic datatypes, ET leverages the functional enumerator FEAT. ET is highly effective at bug finding and has many complimentary benefits. Despite the extensive and continuous testing of the state-of-the-art SMT solvers Z3 and cvc5, ET found 102 bugs, out of which 84 were confirmed and 40 were fixed. Moreover, ET can be used to understand the evolution of solvers. We derive eight grammars realizing all major SMT theories including the booleans, integers, reals, realints, bit-vectors, arrays, floating points, and strings. Using ET, we test all consecutive releases of the SMT solvers Z3 and CVC4/cvc5 from the last six years (61 versions) on 8 million formulas, and 488 million solver calls. Our results suggest improved correctness in recent versions of both solvers but decreased performance in newer releases of Z3 on small timeouts (since z3-4.8.11) and regressions in early cvc5 releases on larger timeouts. Due to its systematic testing and efficiency, we further advocate ET’s use for continuous integration. CCS Concepts: • Software and its engineering → Formal methods.},
	language = {en},
	number = {OOPSLA2},
	urldate = {2025-05-31},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Winterer, Dominik and Su, Zhendong},
	month = oct,
	year = {2024},
	pages = {2378--2401},
}

@inproceedings{barbosa_cvc5_2022,
	address = {Cham},
	title = {cvc5: {A} {Versatile} and {Industrial}-{Strength} {SMT} {Solver}},
	isbn = {978-3-030-99524-9},
	shorttitle = {cvc5},
	doi = {10.1007/978-3-030-99524-9_24},
	abstract = {cvc5 is the latest SMT solver in the cooperating validity checker series and builds on the successful code base of CVC4. This paper serves as a comprehensive system description of cvc5 ’s architectural design and highlights the major features and components introduced since CVC4  1.8. We evaluate cvc5 ’s performance on all benchmarks in SMT-LIB and provide a comparison against CVC4 and Z3.},
	language = {en},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	publisher = {Springer International Publishing},
	author = {Barbosa, Haniel and Barrett, Clark and Brain, Martin and Kremer, Gereon and Lachnitt, Hanna and Mann, Makai and Mohamed, Abdalrhman and Mohamed, Mudathir and Niemetz, Aina and Nötzli, Andres and Ozdemir, Alex and Preiner, Mathias and Reynolds, Andrew and Sheng, Ying and Tinelli, Cesare and Zohar, Yoni},
	editor = {Fisman, Dana and Rosu, Grigore},
	year = {2022},
	keywords = {automated reasoning, constraint solving, cvc5, satisfiability modulo theories},
	pages = {415--442},
}

@inproceedings{blanchette_extending_2011,
	address = {Berlin, Heidelberg},
	title = {Extending {Sledgehammer} with {SMT} {Solvers}},
	isbn = {978-3-642-22438-6},
	doi = {10.1007/978-3-642-22438-6_11},
	abstract = {Sledgehammer is a component of Isabelle/HOL that employs first-order automatic theorem provers (ATPs) to discharge goals arising in interactive proofs. It heuristically selects relevant facts and, if an ATP is successful, produces a snippet that replays the proof in Isabelle. We extended Sledgehammer to invoke satisfiability modulo theories (SMT) solvers as well, exploiting its relevance filter and parallel architecture. Isabelle users are now pleasantly surprised by SMT proofs for problems beyond the ATPs’ reach. Remarkably, the best SMT solver performs better than the best ATP on most of our benchmarks.},
	language = {en},
	booktitle = {Automated {Deduction} – {CADE}-23},
	publisher = {Springer},
	author = {Blanchette, Jasmin Christian and Böhme, Sascha and Paulson, Lawrence C.},
	editor = {Bjørner, Nikolaj and Sofronie-Stokkermans, Viorica},
	year = {2011},
	keywords = {Automatic Theorem Prover, Conjunctive Normal Form, Interactive Proof, Proof Search, Relevance Filter},
	pages = {116--130},
}

@inproceedings{bohme_fast_2010,
	address = {Berlin, Heidelberg},
	title = {Fast {LCF}-{Style} {Proof} {Reconstruction} for {Z3}},
	isbn = {978-3-642-14052-5},
	doi = {10.1007/978-3-642-14052-5_14},
	abstract = {The Satisfiability Modulo Theories (SMT) solver Z3 can generate proofs of unsatisfiability. We present independent reconstruction of these proofs in the theorem provers Isabelle/HOL and HOL4 with particular focus on efficiency. Our highly optimized implementations outperform previous LCF-style proof checkers for SMT, often by orders of magnitude. Detailed performance data shows that LCF-style proof reconstruction can be faster than proof search in Z3.},
	language = {en},
	booktitle = {Interactive {Theorem} {Proving}},
	publisher = {Springer},
	author = {Böhme, Sascha and Weber, Tjark},
	editor = {Kaufmann, Matt and Paulson, Lawrence C.},
	year = {2010},
	pages = {179--194},
}

@inproceedings{bohme_reconstruction_2011,
	address = {Berlin, Heidelberg},
	title = {Reconstruction of {Z3}’s {Bit}-{Vector} {Proofs} in {HOL4} and {Isabelle}/{HOL}},
	isbn = {978-3-642-25379-9},
	doi = {10.1007/978-3-642-25379-9_15},
	abstract = {The Satisfiability Modulo Theories (SMT) solver Z3 can generate proofs of unsatisfiability. We present independent reconstruction of unsatisfiability proofs for bit-vector theories in the theorem provers HOL4 and Isabelle/HOL. Our work shows that LCF-style proof reconstruction for the theory of fixed-size bit-vectors, although difficult because Z3’s proofs provide limited detail, is often possible. We thereby obtain high correctness assurances for Z3’s results, and increase the degree of proof automation for bit-vector problems in HOL4 and Isabelle/HOL.},
	language = {en},
	booktitle = {Certified {Programs} and {Proofs}},
	publisher = {Springer},
	author = {Böhme, Sascha and Fox, Anthony C. J. and Sewell, Thomas and Weber, Tjark},
	editor = {Jouannaud, Jean-Pierre and Shao, Zhong},
	year = {2011},
	pages = {183--198},
}

@inproceedings{moura_lean_2021,
	address = {Berlin, Heidelberg},
	title = {The {Lean} 4 {Theorem} {Prover} and {Programming} {Language}},
	isbn = {978-3-030-79875-8},
	url = {https://doi.org/10.1007/978-3-030-79876-5_37},
	doi = {10.1007/978-3-030-79876-5_37},
	abstract = {Lean 4 is a reimplementation of the Lean interactive theorem prover (ITP) in Lean itself. It addresses many shortcomings of the previous versions and contains many new features. Lean 4 is fully extensible: users can modify and extend the parser, elaborator, tactics, decision procedures, pretty printer, and code generator. The new system has a hygienic macro system custom-built for ITPs. It contains a new typeclass resolution procedure based on tabled resolution, addressing significant performance problems reported by the growing user base. Lean 4 is also an efficient functional programming language based on a novel programming paradigm called functional but in-place. Efficient code generation is crucial for Lean users because many write custom proof automation procedures in Lean itself.},
	urldate = {2025-05-10},
	booktitle = {Automated {Deduction} – {CADE} 28: 28th {International} {Conference} on {Automated} {Deduction}, {Virtual} {Event}, {July} 12–15, 2021, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Moura, Leonardo de and Ullrich, Sebastian},
	month = jul,
	year = {2021},
	pages = {625--635},
}

@article{bhat_verifying_2024,
	title = {Verifying {Peephole} {Rewriting} in {SSA} {Compiler} {IRs}},
	volume = {309},
	copyright = {Creative Commons Attribution 4.0 International license, info:eu-repo/semantics/openAccess},
	issn = {1868-8969},
	url = {https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ITP.2024.9},
	doi = {10.4230/LIPICS.ITP.2024.9},
	abstract = {There is an increasing need for domain-specific reasoning in modern compilers. This has fueled the use of tailored intermediate representations (IRs) based on static single assignment (SSA), like in the MLIR compiler framework. Interactive theorem provers (ITPs) provide strong guarantees for the end-to-end verification of compilers (e.g., CompCert). However, modern compilers and their IRs evolve at a rate that makes proof engineering alongside them prohibitively expensive. Nevertheless, well-scoped push-button automated verification tools such as the Alive peephole verifier for LLVM-IR gained recognition in domains where SMT solvers offer efficient (semi) decision procedures. In this paper, we aim to combine the convenience of automation with the versatility of ITPs for verifying peephole rewrites across domain-specific IRs. We formalize a core calculus for SSA-based IRs that is generic over the IR and covers so-called regions (nested scoping used by many domain-specific IRs in the MLIR ecosystem). Our mechanization in the Lean proof assistant provides a user-friendly frontend for translating MLIR syntax into our calculus. We provide scaffolding for defining and verifying peephole rewrites, offering tactics to eliminate the abstraction overhead of our SSA calculus. We prove correctness theorems about peephole rewriting, as well as two classical program transformations. To evaluate our framework, we consider three use cases from the MLIR ecosystem that cover different levels of abstractions: (1) bitvector rewrites from LLVM, (2) structured control flow, and (3) fully homomorphic encryption. We envision that our mechanization provides a foundation for formally verified rewrites on new domain-specific IRs.},
	language = {en},
	urldate = {2024-11-03},
	journal = {LIPIcs, Volume 309, ITP 2024},
	author = {Bhat, Siddharth and Keizer, Alex and Hughes, Chris and Goens, Andrés and Grosser, Tobias},
	collaborator = {Bertot, Yves and Kutsia, Temur and Norrish, Michael},
	year = {2024},
	note = {Artwork Size: 20 pages, 836196 bytes
ISBN: 9783959773379
Medium: application/pdf
Publisher: Schloss Dagstuhl – Leibniz-Zentrum für Informatik},
	keywords = {Computing methodologies → Theorem proving algorithms, MLIR, SSA, Software and its engineering → Compilers, Software and its engineering → Semantics, Theory of computation → Rewrite systems, compilers, mechanization, peephole rewrites, regions, semantics},
	pages = {9:1--9:20},
}

@inproceedings{hadarean_fine_2015,
	address = {Berlin, Heidelberg},
	series = {{LPAR}-20 2015},
	title = {Fine {Grained} {SMT} {Proofs} for the {Theory} of {Fixed}-{Width} {Bit}-{Vectors}},
	isbn = {978-3-662-48898-0},
	url = {https://doi.org/10.1007/978-3-662-48899-7_24},
	doi = {10.1007/978-3-662-48899-7_24},
	abstract = {Many high-level verification tools rely on SMT solvers to efficiently discharge complex verification conditions. Some applications require more than just a yes/no answer from the solver. For satisfiable quantifier-free problems, a satisfying assignment is a natural artifact. In the unsatisfiable case, an externally checkable proof can serve as a certificate of correctness and can be mined to gain additional insight into the problem. We present a method of encoding and checking SMT-generated proofs for the quantifier-free theory of fixed-width bit-vectors. Proof generation and checking for this theory poses several challenges, especially for proofs based on reductions to propositional logic. Such reductions can result in large resolution subproofs in addition to requiring a proof that the reduction itself is correct. We describe a fine-grained proof system formalized in the LFSC framework that addresses some of these challenges with the use of computational side-conditions. We report results using a proof-producing version of the CVC4 SMT solver on unsatisfiable quantifier-free bit-vector benchmarks from the SMT-LIB benchmark library.},
	urldate = {2025-05-24},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Logic} for {Programming}, {Artificial} {Intelligence}, and {Reasoning} - {Volume} 9450},
	publisher = {Springer-Verlag},
	author = {Hadarean, Liana and Barrett, Clark and Reynolds, Andrew and Tinelli, Cesare and Deters, Morgan},
	month = nov,
	year = {2015},
	pages = {340--355},
}

@article{dutertre_solving_2015,
	title = {Solving {Exists}/{Forall} {Problems} {With} {Yices}},
	abstract = {Yices now includes a solver for Exists/Forall problem. We describe the problem, a general solving algorithm, and a key model-based generalization procedure. We explain the Yices implementation of these algorithms and survey a few applications.},
	language = {en},
	journal = {SMT},
	author = {Dutertre, Bruno},
	year = {2015},
}

@inproceedings{gascon_template-based_2014,
	title = {Template-based circuit understanding},
	url = {https://ieeexplore.ieee.org/document/6987599},
	doi = {10.1109/FMCAD.2014.6987599},
	abstract = {When verifying or reverse-engineering digital circuits, one often wants to identify and understand small components in a larger system. A possible approach is to show that the sub-circuit under investigation is functionally equivalent to a reference implementation. In many cases, this task is difficult as one may not have full information about the mapping between input and output of the two circuits, or because the equivalence depends on settings of control inputs. We propose a template-based approach that automates this process. It extracts a functional description for a low-level combinational circuit by showing it to be equivalent to a reference implementation, while synthesizing an appropriate mapping of input and output signals and setting of control signals. The method relies on solving an exists/forall problem using an SMT solver, and on a pruning technique based on signature computation.},
	urldate = {2025-05-13},
	booktitle = {2014 {Formal} {Methods} in {Computer}-{Aided} {Design} ({FMCAD})},
	author = {Gascón, Adrià and Subramanyan, Pramod and Dutertre, Bruno and Tiwari, Ashish and Jovanović, Dejan and Malik, Sharad},
	month = oct,
	year = {2014},
	keywords = {Boolean functions, Combinational circuits, Context, Hardware design languages, Integrated circuits, Libraries, Wires},
	pages = {83--90},
}

@misc{cheng_efsmt_2013,
	title = {{EFSMT}: {A} {Logical} {Framework} for {Cyber}-{Physical} {Systems}},
	shorttitle = {{EFSMT}},
	url = {http://arxiv.org/abs/1306.3456},
	doi = {10.48550/arXiv.1306.3456},
	abstract = {The design of cyber-physical systems is challenging in that it includes the analysis and synthesis of distributed and embedded real-time systems for controlling, often in a nonlinear way, the environment. We address this challenge with EFSMT, the exists-forall quantified first-order fragment of propositional combinations over constraints (including nonlinear arithmetic), as the logical framework and foundation for analyzing and synthesizing cyber-physical systems. We demonstrate the expressiveness of EFSMT by reducing a number of pivotal verification and synthesis problems to EFSMT. Exemplary problems in this paper include synthesis for robust control via BIBO stability, Lyapunov coefficient finding for nonlinear control systems, distributed priority synthesis for orchestrating system components, and synthesis for hybrid control systems. We are also proposing an algorithm for solving EFSMT problems based on the interplay between two SMT solvers for respectively solving universally and existentially quantified problems. This algorithms builds on commonly used techniques in modern SMT solvers, and generalizes them to quantifier reasoning by counterexample-guided constraint strengthening. The EFSMT solver uses Bernstein polynomials for solving nonlinear arithmetic constraints.},
	urldate = {2025-05-13},
	publisher = {arXiv},
	author = {Cheng, Chih-Hong and Shankar, Natarajan and Ruess, Harald and Bensalem, Saddek},
	month = jun,
	year = {2013},
	note = {arXiv:1306.3456 [cs]},
	keywords = {Computer Science - Logic in Computer Science},
}

@inproceedings{beck_automating_2020,
	title = {Automating the {Development} of {Chosen} {Ciphertext} {Attacks}},
	isbn = {978-1-939133-17-5},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/beck},
	booktitle = {29th {USENIX} {Security} {Symposium} ({USENIX} {Security} 20)},
	publisher = {USENIX Association},
	author = {Beck, Gabrielle and Zinkus, Maximilian and Green, Matthew},
	month = aug,
	year = {2020},
	pages = {1821--1837},
}

@inproceedings{sang_performing_2005,
	address = {Pittsburgh, Pennsylvania},
	series = {{AAAI}'05},
	title = {Performing {Bayesian} inference by weighted model counting},
	isbn = {978-1-57735-236-5},
	abstract = {Over the past decade general satisfiability testing algorithms have proven to be surprisingly effective at solving a wide variety of constraint satisfaction problem, such as planning and scheduling (Kautz and Selman 2003). Solving such NP-complete tasks by "compilation to SAT" has turned out to be an approach that is of both practical and theoretical interest. Recently, (Sang et al. 2004) have shown that state of the art SAT algorithms can be efficiently extended to the harder task of counting the number of models (satisfying assignments) of a formula, by employing a technique called component caching. This paper begins to investigate the question of whether "compilation to model-counting" could be a practical technique for solving real-world \#P-complete problems, in particular Bayesian inference. We describe an efficient translation from Bayesian networks to weighted model counting, extend the best model-counting algorithms to weighted model counting, develop an efficient method for computing all marginals in a single counting pass, and evaluate the approach on computationally challenging reasoning problems.},
	urldate = {2025-05-13},
	booktitle = {Proceedings of the 20th national conference on {Artificial} intelligence - {Volume} 1},
	publisher = {AAAI Press},
	author = {Sang, Tian and Bearne, Paul and Kautz, Henry},
	month = jul,
	year = {2005},
	pages = {475--481},
}

@incollection{franke_optgen_2015,
	address = {Berlin, Heidelberg},
	title = {Optgen: {A} {Generator} for {Local} {Optimizations}},
	volume = {9031},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-662-46662-9 978-3-662-46663-6},
	shorttitle = {Optgen},
	url = {http://link.springer.com/10.1007/978-3-662-46663-6_9},
	abstract = {Every compiler comes with a set of local optimization rules, such as x + 0 → x and x \& x → x, that do not require any global analysis. These rules reﬂect the wisdom of the compiler developers about mathematical identities that hold for the operations of their intermediate representation. Unfortunately, these sets of hand-crafted rules guarantee neither correctness nor completeness. Optgen solves this problem by generating all local optimizations up to a given cost limit. Since Optgen veriﬁes each rule using an SMT solver, it guarantees correctness and completeness of the generated rule set. Using Optgen, we tested the latest versions of GCC, ICC and LLVM and identiﬁed more than 50 missing local optimizations that involve only two operations.},
	language = {en},
	urldate = {2024-11-12},
	booktitle = {Compiler {Construction}},
	publisher = {Springer Berlin Heidelberg},
	author = {Buchwald, Sebastian},
	editor = {Franke, Björn},
	year = {2015},
	doi = {10.1007/978-3-662-46663-6_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {171--189},
}

@article{menendez_alive-infer_2017,
	title = {Alive-{Infer}: data-driven precondition inference for peephole optimizations in {LLVM}},
	volume = {52},
	issn = {0362-1340},
	shorttitle = {Alive-{Infer}},
	url = {https://dl.acm.org/doi/10.1145/3140587.3062372},
	doi = {10.1145/3140587.3062372},
	abstract = {Peephole optimizations are a common source of compiler bugs. Compiler developers typically transform an incorrect peephole optimization into a valid one by strengthening the precondition. This process is challenging and tedious. This paper proposes Alive-Infer, a data-driven approach that infers preconditions for peephole optimizations expressed in Alive. Alive-Infer generates positive and negative examples for an optimization, enumerates predicates on-demand, and learns a set of predicates that separate the positive and negative examples. Alive-Infer repeats this process until it finds a precondition that ensures the validity of the optimization. Alive-Infer reports both a weakest precondition and a set of succinct partial preconditions to the developer. Our prototype generates preconditions that are weaker than LLVM’s preconditions for 73 optimizations in the Alive suite. We also demonstrate the applicability of this technique to generalize 54 optimization patterns generated by Souper, an LLVM IR–based superoptimizer.},
	number = {6},
	urldate = {2025-05-16},
	journal = {SIGPLAN Not.},
	author = {Menendez, David and Nagarakatte, Santosh},
	month = jun,
	year = {2017},
	pages = {49--63},
}

@article{tate_generating_2010,
	title = {Generating compiler optimizations from proofs},
	volume = {45},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/1707801.1706345},
	doi = {10.1145/1707801.1706345},
	abstract = {We present an automated technique for generating compiler optimizations from examples of concrete programs before and after improvements have been made to them. The key technical insight of our technique is that a proof of equivalence between the original and transformed concrete programs informs us which aspects of the programs are important and which can be discarded. Our technique therefore uses these proofs, which can be produced by translation validation or a proof-carrying compiler, as a guide to generalize the original and transformed programs into broadly applicable optimization rules.We present a category-theoretic formalization of our proof generalization technique. This abstraction makes our technique applicable to logics besides our own. In particular, we demonstrate how our technique can also be used to learn query optimizations for relational databases or to aid programmers in debugging type errors.Finally, we show experimentally that our technique enables programmers to train a compiler with application-specific optimizations by providing concrete examples of original programs and the desired transformed programs. We also show how it enables a compiler to learn efficient-to-run optimizations from expensive-to-run super-optimizers.},
	number = {1},
	urldate = {2024-11-03},
	journal = {ACM SIGPLAN Notices},
	author = {Tate, Ross and Stepp, Michael and Lerner, Sorin},
	month = jan,
	year = {2010},
	pages = {389--402},
}

@incollection{hutchison_tale_2014,
	address = {Cham},
	title = {A {Tale} of {Two} {Solvers}: {Eager} and {Lazy} {Approaches} to {Bit}-{Vectors}},
	volume = {8559},
	isbn = {978-3-319-08866-2 978-3-319-08867-9},
	shorttitle = {A {Tale} of {Two} {Solvers}},
	url = {http://link.springer.com/10.1007/978-3-319-08867-9_45},
	abstract = {The standard method for deciding bit-vector constraints is via eager reduction to propositional logic. This is usually done after ﬁrst applying powerful rewrite techniques. While often efﬁcient in practice, this method does not scale on problems for which top-level rewrites cannot reduce the problem size sufﬁciently. A lazy solver can target such problems by doing many satisﬁability checks, each of which only reasons about a small subset of the problem. In addition, the lazy approach enables a wide range of optimization techniques that are not available to the eager approach. In this paper we describe the architecture and features of our lazy solver (LBV). We provide a comparative analysis of the eager and lazy approaches, and show how they are complementary in terms of the types of problems they can efﬁciently solve. For this reason, we propose a portfolio approach that runs a lazy and eager solver in parallel. Our empirical evaluation shows that the lazy solver can solve problems none of the eager solvers can and that the portfolio solver outperforms other solvers both in terms of total number of problems solved and the time taken to solve them.},
	language = {en},
	urldate = {2025-05-08},
	booktitle = {Computer {Aided} {Verification}},
	publisher = {Springer International Publishing},
	author = {Hadarean, Liana and Bansal, Kshitij and Jovanović, Dejan and Barrett, Clark and Tinelli, Cesare},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Biere, Armin and Bloem, Roderick},
	year = {2014},
	doi = {10.1007/978-3-319-08867-9_45},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {680--695},
}

@article{massalin_superoptimizer_1987,
	title = {Superoptimizer: a look at the smallest program},
	volume = {15},
	issn = {0163-5964},
	shorttitle = {Superoptimizer},
	url = {https://dl.acm.org/doi/10.1145/36177.36194},
	doi = {10.1145/36177.36194},
	abstract = {Given an instruction set, the superoptimizer finds the shortest program to compute a function. Startling programs have been generated, many of them engaging in convoluted bit-fiddling bearing little resemblance to the source programs which defined the functions. The key idea in the superoptimizer is a probabilistic test that makes exhaustive searches practical for programs of useful size. The search space is defined by the processor's instruction set, which may include the whole set, but it is typically restricted to a subset. By constraining the instructions and observing the effect on the output program, one can gain insight into the design of instruction sets. In addition, superoptimized programs may be used by peephole optimizers to improve the quality of generated code, or by assembly language programmers to improve manually written code.},
	number = {5},
	urldate = {2025-05-09},
	journal = {ACM SIGARCH Computer Architecture News},
	author = {Massalin, Henry},
	month = oct,
	year = {1987},
	pages = {122--126},
}

@article{schkufza_stochastic_2013,
	title = {Stochastic superoptimization},
	volume = {48},
	issn = {0362-1340},
	url = {https://dl.acm.org/doi/10.1145/2499368.2451150},
	doi = {10.1145/2499368.2451150},
	abstract = {We formulate the loop-free binary superoptimization task as a stochastic search problem. The competing constraints of transformation correctness and performance improvement are encoded as terms in a cost function, and a Markov Chain Monte Carlo sampler is used to rapidly explore the space of all possible programs to find one that is an optimization of a given target program. Although our method sacrifices completeness, the scope of programs we are able to consider, and the resulting quality of the programs that we produce, far exceed those of existing superoptimizers. Beginning from binaries compiled by llvm -O0 for 64-bit x86, our prototype implementation, STOKE, is able to produce programs which either match or outperform the code produced by gcc -O3, icc -O3, and in some cases, expert handwritten assembly.},
	number = {4},
	urldate = {2025-05-09},
	journal = {SIGPLAN Not.},
	author = {Schkufza, Eric and Sharma, Rahul and Aiken, Alex},
	month = mar,
	year = {2013},
	pages = {305--316},
}

@inproceedings{phothilimthana_scaling_2016,
	address = {New York, NY, USA},
	series = {{ASPLOS} '16},
	title = {Scaling up {Superoptimization}},
	isbn = {978-1-4503-4091-5},
	url = {https://dl.acm.org/doi/10.1145/2872362.2872387},
	doi = {10.1145/2872362.2872387},
	abstract = {Developing a code optimizer is challenging, especially for new, idiosyncratic ISAs. Superoptimization can, in principle, discover machine-specific optimizations automatically by searching the space of all instruction sequences. If we can increase the size of code fragments a superoptimizer can optimize, we will be able to discover more optimizations. We develop LENS, a search algorithm that increases the size of code a superoptimizer can synthesize by rapidly pruning away invalid candidate programs. Pruning is achieved by selectively refining the abstraction under which candidates are considered equivalent, only in the promising part of the candidate space. LENS also uses a bidirectional search strategy to prune the candidate space from both forward and backward directions. These pruning strategies allow LENS to solve twice as many benchmarks as existing enumerative search algorithms, while LENS is about 11-times faster.Additionally, we increase the effective size of the superoptimized fragments by relaxing the correctness condition using contexts (surrounding code). Finally, we combine LENS with complementary search techniques into a cooperative superoptimizer, which exploits the stochastic search to make random jumps in a large candidate space, and a symbolic (SAT-solver-based) search to synthesize arbitrary constants. While existing superoptimizers consistently solve 9--16 out of 32 benchmarks, the cooperative superoptimizer solves 29 benchmarks. It can synthesize code fragments that are up to 82\% faster than code generated by gcc -O3 from WiBench and MiBench.},
	urldate = {2025-06-07},
	booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Phothilimthana, Phitchaya Mangpo and Thakur, Aditya and Bodik, Rastislav and Dhurjati, Dinakar},
	month = mar,
	year = {2016},
	pages = {297--310},
}

@inproceedings{jia_taso_2019,
	address = {Huntsville Ontario Canada},
	title = {{TASO}: optimizing deep learning computation with automatic generation of graph substitutions},
	isbn = {978-1-4503-6873-5},
	shorttitle = {{TASO}},
	url = {https://dl.acm.org/doi/10.1145/3341301.3359630},
	doi = {10.1145/3341301.3359630},
	abstract = {Existing deep neural network (DNN) frameworks optimize the computation graph of a DNN by applying graph transformations manually designed by human experts. This approach misses possible graph optimizations and is difficult to scale, as new DNN operators are introduced on a regular basis. We propose TASO, the first DNN computation graph optimizer that automatically generates graph substitutions. TASO takes as input a list of operator specifications and generates candidate substitutions using the given operators as basic building blocks. All generated substitutions are formally verified against the operator specifications using an automated theorem prover. To optimize a given DNN computation graph, TASO performs a cost-based backtracking search, applying the substitutions to find an optimized graph, which can be directly used by existing DNN frameworks. Our evaluation on five real-world DNN architectures shows that TASO outperforms existing DNN frameworks by up to 2.8×, while requiring significantly less human effort. For example, TensorFlow currently contains approximately 53,000 lines of manual optimization rules, while the operator specifications needed by TASO are only 1,400 lines of code.},
	language = {en},
	urldate = {2024-11-13},
	booktitle = {Proceedings of the 27th {ACM} {Symposium} on {Operating} {Systems} {Principles}},
	publisher = {ACM},
	author = {Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex},
	month = oct,
	year = {2019},
	pages = {47--62},
}

@inproceedings{wang_pet_2021,
	title = {\{{PET}\}: {Optimizing} {Tensor} {Programs} with {Partially} {Equivalent} {Transformations} and {Automated} {Corrections}},
	isbn = {978-1-939133-22-9},
	shorttitle = {\{{PET}\}},
	url = {https://www.usenix.org/conference/osdi21/presentation/wang},
	language = {en},
	urldate = {2024-11-15},
	booktitle = {15th {USENIX} {Symposium} on {Operating} {Systems} {Design} and {Implementation} ({OSDI} 21)},
	publisher = {USENIX Association},
	author = {Wang, Haojie and Zhai, Jidong and Gao, Mingyu and Ma, Zixuan and Tang, Shizhi and Zheng, Liyan and Li, Yuanzhi and Rong, Kaiyuan and Chen, Yuanyong and Jia, Zhihao},
	year = {2021},
	pages = {37--54},
}

@inproceedings{wu_mirage_2025,
	address = {Boston, MA},
	title = {Mirage: {A} {Multi}-{Level} {Superoptimizer} for {Tensor} {Programs}},
	booktitle = {19th {USENIX} {Symposium} on {Operating} {Systems} {Design} and {Implementation} ({OSDI} 25)},
	publisher = {USENIX Association},
	author = {Wu, Mengdi and Cheng, Xinhao and Liu, Shengyu and Shi, Chunan and Ji, Jianan and Ao, Kit and Velliengiri, Praveen and Miao, Xupeng and Padon, Oded and Jia, Zhihao},
	month = jul,
	year = {2025},
}

@inproceedings{padhi_data-driven_2016,
	address = {New York, NY, USA},
	series = {{PLDI} '16},
	title = {Data-driven precondition inference with learned features},
	isbn = {978-1-4503-4261-2},
	url = {https://dl.acm.org/doi/10.1145/2908080.2908099},
	doi = {10.1145/2908080.2908099},
	abstract = {We extend the data-driven approach to inferring preconditions for code from a set of test executions. Prior work requires a fixed set of features, atomic predicates that define the search space of possible preconditions, to be specified in advance. In contrast, we introduce a technique for on-demand feature learning, which automatically expands the search space of candidate preconditions in a targeted manner as necessary. We have instantiated our approach in a tool called PIE. In addition to making precondition inference more expressive, we show how to apply our feature-learning technique to the setting of data-driven loop invariant inference. We evaluate our approach by using PIE to infer rich preconditions for black-box OCaml library functions and using our loop-invariant inference algorithm as part of an automatic program verifier for C++ programs.},
	urldate = {2025-05-16},
	booktitle = {Proceedings of the 37th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Padhi, Saswat and Sharma, Rahul and Millstein, Todd},
	month = jun,
	year = {2016},
	pages = {42--56},
}

@inproceedings{lopes_weakest_2014,
	address = {Berlin, Heidelberg},
	title = {Weakest {Precondition} {Synthesis} for {Compiler} {Optimizations}},
	isbn = {978-3-642-54013-4},
	doi = {10.1007/978-3-642-54013-4_12},
	abstract = {Compiler optimizations play an increasingly important role in code generation. This is especially true with the advent of resourcelimited mobile devices. We rely on compiler optimizations to improve performance, reduce code size, and reduce power consumption of our programs.},
	language = {en},
	booktitle = {Verification, {Model} {Checking}, and {Abstract} {Interpretation}},
	publisher = {Springer},
	author = {Lopes, Nuno P. and Monteiro, José},
	editor = {McMillan, Kenneth L. and Rival, Xavier},
	year = {2014},
	keywords = {Automatic Synthesis, Boolean Variable, Context Variable, Target Template, Transformation Function},
	pages = {203--221},
}

@inproceedings{darwiche_new_2004,
	address = {NLD},
	series = {{ECAI}'04},
	title = {New advances in compiling {CNF} to {Decomposable} {Negation} {Normal} form},
	isbn = {978-1-58603-452-8},
	abstract = {We describe a new algorithm for compiling conjunctive normal form (CNF) into Deterministic Decomposable Negation Normal (d-DNNF), which is a tractable logical form that permits model counting in polynomial time. The new implementation is based on latest techniques from both the SAT and OBDD literatures, and appears to be orders of magnitude more efficient than previous algorithms for this purpose. We compare our compiler experimentally to state of the art model counters, OBDD compilers, and previous CNF2dDNNF compilers.},
	urldate = {2025-05-16},
	booktitle = {Proceedings of the 16th {European} {Conference} on {Artificial} {Intelligence}},
	publisher = {IOS Press},
	author = {Darwiche, Adnan},
	month = aug,
	year = {2004},
	pages = {318--322},
}

@inproceedings{sang_combining_2004,
	address = {Vancouver, BC, Canada},
	title = {Combining {Component} {Caching} and {Clause} {Learning} for {Effective} {Model} {Counting}},
	language = {en},
	author = {Sang, Tian and Bacchus, Fahiem and Beame, Paul and Kautz, Henry and Pitassi, Toniann},
	year = {2004},
}

@inproceedings{thurley_sharpsat_2006,
	address = {Berlin, Heidelberg},
	title = {{sharpSAT} – {Counting} {Models} with {Advanced} {Component} {Caching} and {Implicit} {BCP}},
	isbn = {978-3-540-37207-3},
	doi = {10.1007/11814948_38},
	abstract = {We introduce sharpSAT, a new \#SAT solver that is based on the well known DPLL algorithm and techniques from SAT and \#SAT solvers. Most importantly, we introduce an entirely new approach of coding components, which reduces the cache size by at least one order of magnitude, and a new cache management scheme. Furthermore, we apply a well known look ahead based on BCP in a manner that is well suited for \#SAT solving. We show that these techniques are highly beneficial, especially on large structured instances, such that our solver performs significantly better than other \#SAT solvers.},
	language = {en},
	booktitle = {Theory and {Applications} of {Satisfiability} {Testing} - {SAT} 2006},
	publisher = {Springer},
	author = {Thurley, Marc},
	editor = {Biere, Armin and Gomes, Carla P.},
	year = {2006},
	keywords = {Bound Model Check, Branch Variable, Cache Size, Conjunctive Normal Form, Unit Clause},
	pages = {424--429},
}

@inproceedings{chakraborty_scalable_2013,
	address = {Berlin, Heidelberg},
	title = {A {Scalable} {Approximate} {Model} {Counter}},
	isbn = {978-3-642-40627-0},
	doi = {10.1007/978-3-642-40627-0_18},
	abstract = {Propositional model counting (\#SAT), i.e., counting the number of satisfying assignments of a propositional formula, is a problem of significant theoretical and practical interest. Due to the inherent complexity of the problem, approximate model counting, which counts the number of satisfying assignments to within given tolerance and confidence level, was proposed as a practical alternative to exact model counting. Yet, approximate model counting has been studied essentially only theoretically. The only reported implementation of approximate model counting, due to Karp and Luby, worked only for DNF formulas. A few existing tools for CNF formulas are bounding model counters; they can handle realistic problem sizes, but fall short of providing counts within given tolerance and confidence, and, thus, are not approximate model counters.},
	language = {en},
	booktitle = {Principles and {Practice} of {Constraint} {Programming}},
	publisher = {Springer},
	author = {Chakraborty, Supratik and Meel, Kuldeep S. and Vardi, Moshe Y.},
	editor = {Schulte, Christian},
	year = {2013},
	pages = {200--216},
}

@inproceedings{dalla_machine_2024,
	address = {Los Alamitos, CA, USA},
	title = {A {Machine} {Learning} {Approach} to {Model} {Counting}},
	url = {https://doi.ieeecomputersociety.org/10.1109/ICTAI62512.2024.00128},
	doi = {10.1109/ICTAI62512.2024.00128},
	abstract = {Model counting (\#SAT) is the problem of computing the number of satisfying assignments for a given Boolean formula. It has a significant theoretical and practical interest. Tackling it can be challenging since the number of potential solution grows exponentially with the number of variables. Due to the inherent complexity of the problem, approaches to approximate model counting have been developed as a practical alternative. These methods extract the number of solutions within user-specified tolerance and confidence levels and in a fraction of the time required by exact model counters. However, even these methods require extensive computations, restricting their applicability to relatively small instances. In this paper, we propose a new approximate machine learning model counter that overcome this limitation. Predicting the number of solutions can be seen as a regression problem. We deploy an array of machine learning techniques trained to infer the approximate number of solutions based on statistical features extracted from a SAT propositional formula. Extensive numerical experiments performed on synthetic crafted and benchmark datasets show that learning approaches can provide a good approximation of the number of solutions with a much lower computational time and resource cost than the state-of-the-art approximate and exact model counters, making it possible to approximate the model count of instances previously out of reach. We then investigated the structural factors that lead to a high model count using AI explainability approaches.},
	booktitle = {2024 {IEEE} 36th {International} {Conference} on {Tools} with {Artificial} {Intelligence} ({ICTAI})},
	publisher = {IEEE Computer Society},
	author = {Dalla, Marco and Visentin, Andrea and O'Sullivan, Barry},
	month = oct,
	year = {2024},
	keywords = {Accuracy, Computational modeling, Deep learning, Estimation, Feature extraction, Learning (artificial intelligence), Machine learning algorithms, Measurement, Numerical models, Predictive models},
	pages = {881--888},
}

@misc{noauthor_bvdecide_nodate,
	title = {{BVDecide} {Tactic} in {Lean} 4},
	url = {https://leanprover-community.github.io/mathlib4_docs/Lean/Elab/Tactic/BVDecide.html},
	urldate = {2025-06-01},
	journal = {Lean.Elab.Tactic.BVDecide},
}

@book{nipkow_isabellehol_2002,
	address = {Berlin, Heidelberg},
	title = {Isabelle/{HOL}: a proof assistant for higher-order logic},
	isbn = {978-3-540-43376-7},
	shorttitle = {Isabelle/{HOL}},
	publisher = {Springer-Verlag},
	author = {Nipkow, Tobias and Wenzel, Markus and Paulson, Lawrence C.},
	year = {2002},
}

@inproceedings{ekici_smtcoq_2017,
	address = {Heidelberg, Germany},
	title = {{SMTCoq}: {A} plug-in for integrating {SMT} solvers into {Coq}},
	url = {https://hal.science/hal-01669345},
	booktitle = {Computer {Aided} {Verification} - 29th {International} {Conference}},
	author = {Ekici, Burak and Mebsout, Alain and Tinelli, Cesare and Keller, Chantal and Katz, Guy and Reynolds, Andrew and Barrett, Clark},
	month = jul,
	year = {2017},
}

@book{bertot_interactive_2013,
	title = {Interactive theorem proving and program development: {Coq}’{Art}: the calculus of inductive constructions},
	publisher = {Springer Science \& Business Media},
	author = {Bertot, Yves and Castéran, Pierre},
	year = {2013},
}

@misc{mohamed_lean-smt_2025,
	title = {Lean-{SMT}: {An} {SMT} tactic for discharging proof goals in {Lean}},
	shorttitle = {Lean-{SMT}},
	url = {http://arxiv.org/abs/2505.15796},
	doi = {10.48550/arXiv.2505.15796},
	abstract = {Lean is an increasingly popular proof assistant based on dependent type theory. Despite its success, it still lacks important automation features present in more seasoned proof assistants, such as the Sledgehammer tactic in Isabelle/HOL. A key aspect of Sledgehammer is the use of proof-producing SMT solvers to prove a translated proof goal and the reconstruction of the resulting proof into valid justifications for the original goal. We present Lean-SMT, a tactic providing this functionality in Lean. We detail how the tactic converts Lean goals into SMT problems and, more importantly, how it reconstructs SMT proofs into native Lean proofs. We evaluate the tactic on established benchmarks used to evaluate Sledgehammer's SMT integration, with promising results. We also evaluate Lean-SMT as a standalone proof checker for proofs of SMT-LIB problems. We show that Lean-SMT offers a smaller trusted core without sacrificing too much performance.},
	urldate = {2025-06-09},
	publisher = {arXiv},
	author = {Mohamed, Abdalrhman and Mascarenhas, Tomaz and Khan, Harun and Barbosa, Haniel and Reynolds, Andrew and Qian, Yicheng and Tinelli, Cesare and Barrett, Clark},
	month = may,
	year = {2025},
	note = {arXiv:2505.15796 [cs]},
	keywords = {Computer Science - Logic in Computer Science},
}
